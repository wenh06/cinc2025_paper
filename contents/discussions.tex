\section{Discussion and Conclusions}
\label{sec:discu}

% almost finished.

The Challenge scores presented in Table~\ref{tab:scores} indicate that our proposed method is effective for Chagas screening from ECGs, albeit with substantial room for improvement. The result demonstrates our model's ability to learn diagnostically relevant features from ECGs for this task under scarce and noisy supervision. This is achieved through reliability-aware label smoothing, which incorporates both label provenance and reliability instead of treating all positive labels uniformly. Together with the asymmetric loss and strategic upsampling, these results indicate that explicitly modeling label reliability helps stabilize the learning process more effectively than introducing additional architectural complexity. Overall, our approach offers a scalable and resource-efficient solution and aligns well with the Challenge's objective of identifying high-risk individuals under limited serological testing capacity.

% However, the performance gap between our internal hold-out subset and the hidden test set suggests two primary limitations. First, the reliability weights (smoothing factors) were pre-defined based on label provenance rather than learned from data. This static assignment cannot capture the inherent heterogeneity of label quality within each source. Second, positive upsampling was applied uniformly at the dataset level using fixed factors. This strategy overlooks variations in individual sample difficulty and does not adapt to the model's evolving confidence during training. Furthermore, we did not make use of the additional labels available in some datasets, such as arrhythmia labels, to design and implement auxiliary learning tasks that could have enhanced the performance of the main task, Chagas screening.

However, the performance gap between our internal hold-out subset and the hidden test set indicates limitations in our method's generalization capability. This is also reflected in the absolute performance, where our Challenge score is substantially behind those of the top-performing teams (e.g., 0.323, 0.283, 0.280). The leading approaches generally leveraged some form of large-scale representation learning, such as pre-trained Vision Transformer foundation models or self-supervised learning on extensive ECG datasets. These strategies focus on building robust, general-purpose feature extractors. In contrast, our work prioritized a distinct niche by designing a resource-efficient pipeline that explicitly addresses the challenges of label noise and scarcity through reliability-aware smoothing and asymmetric loss, without relying on massive pretraining. While this choice enhances practicality and stability under noisy supervision, it appears that the representational capacity of our directly-trained model is ultimately lower, limiting its ability to generalize as effectively as the foundation-model-based approaches. Furthermore, our static assignment of reliability weights, which cannot adapt to instance-specific label quality, represents another limitation compared to more dynamic or learned weighting schemes.

% Future research directions will primarily focus on developing a self-adaptive supervision framework. This includes dynamic weighting schemes for learning instance-specific reliability scores, moving beyond static smoothing factors, and adaptive sampling strategies that respond to the model's evolving confidence during training, offering a promising alternative to fixed upsampling factors. Data augmentation techniques, such as CutMix \cite{yun2019cutmix} and SMOTE \cite{Chawla_2002_SMOTE}, could be applied to further expand and diversify the positive samples, thereby enhancing the model's robustness against overfitting and improving generalization to underrepresented patterns. Additionally, a multi-task learning framework leveraging auxiliary arrhythmia labels could enhance feature representation and improve generalization for the primary Chagas screening task. Exploring self-supervised pre-training on large-scale unlabeled ECG data also represents a promising direction to learn more transferable representations before fine-tuning on the target task.

Building upon these insights, future research will focus on bridging the generalization gap while retaining our focus on learning under weak supervision. A primary direction is the self-adaptive supervision framework. This includes dynamic weighting schemes, moving beyond our current static factors, and adaptive sampling strategies that respond to the model's evolving confidence during training. To further improve robustness and handle class imbalance, advanced data augmentation techniques such as CutMix \cite{yun2019cutmix} and SMOTE \cite{Chawla_2002_SMOTE} will be explored to diversify the limited positive samples, with a particular focus on ECG-specific transformations like lead-wise masking. Furthermore, inspired by the success of representation learning, we plan to explore self-supervised pre-training on large-scale unlabeled ECG data as a pivotal step to learn more transferable representations before fine-tuning. This direction, while computationally more demanding, addresses a key limitation identified in our current work. Additionally, a multi-task learning framework leveraging auxiliary arrhythmia labels could be integrated to impose clinically meaningful constraints and enhance the feature representation for the primary Chagas screening task.
